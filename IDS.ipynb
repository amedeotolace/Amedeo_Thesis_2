{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMk57a4i/klFvmb1PVdO0T2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amedeotolace/Amedeo_Thesis_2/blob/main/IDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow_datasets\n",
        "!pip install tensorflow tensorflow_decision_forests\n",
        "!pip install datapackage\n",
        "!pip install pandas\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "ONen2a1Njd1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datapackage\n",
        "import pandas as pd\n",
        "\n",
        "data_url = 'https://datahub.io/machine-learning/kddcup99/datapackage.json'\n",
        "names = [\n",
        "    'count',\n",
        "    'diff_srv_rate',\n",
        "    'dst_bytes',\n",
        "    'dst_host_count',\n",
        "    'dst_host_diff_srv_rate',\n",
        "    'dst_host_rerror_rate',\n",
        "    'dst_host_same_src_port_rate',\n",
        "    'dst_host_same_srv_rate',\n",
        "    'dst_host_serror_rate',\n",
        "    'dst_host_srv_count',\n",
        "    'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_srv_rerror_rate',\n",
        "    'dst_host_srv_serror_rate',\n",
        "    'duration',\n",
        "    'flag',\n",
        "    'hot',\n",
        "    'is_guest_login',\n",
        "    'is_hot_login',\n",
        "    'label',\n",
        "    'land',\n",
        "    'logged_in',\n",
        "    'num_access_files',\n",
        "    'num_compromised',\n",
        "    'num_failed_logins',\n",
        "    'num_file_creations',\n",
        "    'num_outbound_cmds',\n",
        "    'num_root',\n",
        "    'num_shells',\n",
        "    'protocol_type',\n",
        "    'rerror_rate',\n",
        "    'root_shell',\n",
        "    'same_srv_rate',\n",
        "    'serror_rate',\n",
        "    'service',\n",
        "    'src_bytes',\n",
        "    'srv_count',\n",
        "    'srv_diff_host_rate',\n",
        "    'srv_rerror_rate',\n",
        "    'srv_serror_rate',\n",
        "    'su_attempted',\n",
        "    'urgent',\n",
        "    'wrong_fragment',\n",
        "]\n",
        "\n",
        "# to load Data Package into storage\n",
        "package = datapackage.Package(data_url)\n",
        "\n",
        "# to load only tabular data\n",
        "resources = package.resources\n",
        "for resource in resources:\n",
        "    if resource.tabular:\n",
        "        dataset_df = pd.read_csv(resource.descriptor['path'], header=None, names=names)\n",
        "        print (dataset_df)"
      ],
      "metadata": {
        "id": "bKmHiqQkYnJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split del dataset in train & test\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  \"\"\"Divide un pandas dataframe, in questo caso suddiviamo il dataset caricato in 2\"\"\"\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "\n",
        "train_ds_pd, test_ds_pd = split_dataset(dataset_df)\n",
        "\n",
        "#Lunghezza dei dataset di test e di train\n",
        "\"\"\"print(\"{} examples in training, {} examples for testing.\".format(\n",
        "    len(train_ds_pd), len(test_ds_pd)))\n",
        "\"\"\"\n",
        "print(test_ds_pd.T)"
      ],
      "metadata": {
        "id": "Ndn6oCr5OCZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import pandas as pd\n",
        "\n",
        "#Convertiamo da Pandas dataframe ad un dataset Tensorflow\n",
        "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label='protocol_type')\n",
        "test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label='protocol_type')\n",
        "\n",
        "# Random Forest model.\n",
        "model = tfdf.keras.RandomForestModel()\n",
        "model.fit(train_ds)\n",
        "\n",
        "# Summary of the model structure.\n",
        "model.summary()\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(test_ds)\n",
        "\n",
        "# Export the model to a SavedModel.\n",
        "model.save(\"project/model\")"
      ],
      "metadata": {
        "id": "t6P5CCZVklGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow numpy scikit-learn\n"
      ],
      "metadata": {
        "id": "yr-4PyxquAf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_df)"
      ],
      "metadata": {
        "id": "obdulHm1vuoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Input, Dense\n",
        "\n",
        "# Convert categorical features to one-hot encoding\n",
        "df = pd.get_dummies(dataset_df, columns=[\"protocol_type\", \"service\", \"flag\"])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "OHYFOO6AwFkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "X = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "6LPlvomIwvO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the non-symmetric deep autoencoder model\n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "encoded = Dense(64, activation='relu')(input_layer)\n",
        "encoded = Dense(32, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(64, activation='relu')(encoded)\n",
        "decoded = Dense(X.shape[1], activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(input_layer, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, shuffle=True, validation_data=(X_test, X_test))\n",
        "\n",
        "# Encode the data\n",
        "encoder = Model(input_layer, encoded)\n",
        "encoded_train = encoder.predict(X_train)\n",
        "encoded_test = encoder.predict(X_test)\n",
        "\n",
        "# Use the encoded data for classification (for example, with a simple classifier)\n",
        "classifier_input = Input(shape=(32,))\n",
        "classifier_output = Dense(1, activation='sigmoid')(classifier_input)\n",
        "classifier = Model(classifier_input, classifier_output)\n",
        "\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(encoded_train, y_train, epochs=10, batch_size=32, shuffle=True, validation_data=(encoded_test, y_test))\n",
        "\n",
        "# Evaluate the classifier on the test data\n",
        "y_pred = classifier.predict(encoded_test)\n",
        "y_pred_binary = np.round(y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "gvI0OczvvQUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}